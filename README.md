# ðŸŽ­ CelebA Conditional Diffusion Model

=====================================================

ðŸš€ A PyTorch implementation of a **DDPM-based conditional diffusion model** for **controllable face generation** on the CelebA dataset (64Ã—64 resolution).

Developed for the **Generative AI (MSc in Computer Engineering)** course at the **University of Salerno**.

> Demonstrates practical experience in diffusion models, conditional generative modeling, UNet architectures, attention mechanisms, EMA stabilization and end-to-end training pipelines.

---

## ðŸ“Œ Overview

This project implements a **conditional Denoising Diffusion Probabilistic Model (DDPM)** capable of generating face images conditioned on three semantic attributes:

* ðŸ‘¨ **Male / Female**
* ðŸ˜Š **Smiling / Not Smiling**
* ðŸ‘¶ **Young / Not Young**

The model learns to generate realistic 64Ã—64 face images from pure Gaussian noise, guided by attribute conditioning.

---

## ðŸ§  Model Architecture

The generative backbone is a **Conditional U-Net** trained to predict noise Îµâ‚œ.

### ðŸ”¹ Core Components

* Sinusoidal **Time Embedding**
* Learnable **Condition Embedding**
* **FiLM modulation** inside residual blocks
* Multi-scale **Self-Attention** (16Ã—16 and 8Ã—8)
* Linear beta schedule (1000 diffusion steps)
* **EMA (Exponential Moving Average)** stabilization

### ðŸ”„ Diffusion Process

Forward Process:

```
xâ‚€ â†’ xâ‚œ (progressive noise injection)
```

Reverse Process (learned):

```
xâ‚œ â†’ xâ‚œâ‚‹â‚ â†’ ... â†’ xâ‚€
```

The model predicts noise at each timestep and reconstructs the clean image via iterative denoising.

---

## ðŸ— Architecture Details

**Input Resolution:** 64Ã—64  
**Base Channels:** 128  
**Timesteps:** 1000  
**Optimizer:** AdamW  
**Loss:** MSE (noise prediction objective)

The U-Net includes:

* Downsampling: 64 â†’ 32 â†’ 16 â†’ 8
* Bottleneck at 8Ã—8
* Symmetric decoder with skip connections
* Attention layers at 16Ã—16 and 8Ã—8 resolutions
* FiLM-based conditioning (time + attributes)

---

## ðŸ“Š Conditioning Strategy

Each sample is conditioned on a 3-dimensional binary vector:

```
[Male, Smiling, Young]
```

Example:

```
[1, 1, 0] â†’ Male, Smiling, Not Young
```

All 8 possible attribute combinations are supported during sampling.

---

## ðŸ“‚ Repository Structure

```
ðŸ“¦ celeba-conditional-diffusion  
â”œâ”€â”€ architecture.py        # Conditional U-Net with Attention + FiLM  
â”œâ”€â”€ training_lite.py        # DDPM scheduler + training loop + EMA  
â”œâ”€â”€ inference.py           # Conditional sampling script  
â”‚  
â”œâ”€â”€ weights/            # Saved model weights  
â”‚  
â””â”€â”€ README.md
```

---

## ðŸ§ª Training Pipeline

### 1ï¸âƒ£ Dataset

Dataset used: **CelebA**

Attributes extracted:

* #20 â†’ Male
* #31 â†’ Smiling
* #39 â†’ Young

Images are:

* Resized to 64Ã—64
* Center cropped
* Normalized to [-1, 1]

---

### 2ï¸âƒ£ Training

Run:

```
python training_lite.py
```

Features:

* Random timestep sampling
* Forward diffusion noise injection
* Noise prediction objective
* Gradient clipping
* EMA model tracking
* Periodic sample generation
* Automatic checkpoint saving

Checkpoints saved in:

```
weights/latest.pt
```

---

### 3ï¸âƒ£ Inference

Edit the conditioning vector inside:

```
inference.py
```

Then run:

```
python inference.py
```

The script:

* Loads the trained checkpoint
* Restores the scheduler
* Generates N samples with the same conditioning
* Saves a grid image in `/generated`

---

## ðŸ”¬ Technologies Used

* PyTorch
* Torchvision
* CelebA Dataset
* UNet Architecture
* Self-Attention
* DDPM Scheduler (custom implementation)
* EMA (Stabilized Training)
* AdamW Optimizer

---

## ðŸŽ“ Academic Context

Developed as a **Project Work** for:

**Generative AI â€“ MSc in Computer Engineering**  
University of Salerno  
Academic Year 2025/2026

The original assignment required multiple generative approaches; this repository contains the **Diffusion Model implementation**.

The other 2 implementations can be found [here]().

---

## ðŸ’¡ Key Challenges Addressed

* Stable diffusion training from scratch
* Conditioning injection inside UNet blocks
* Attention integration at low resolutions
* Reverse diffusion numerical stability
* EMA-based sampling stabilization
* Memory-efficient training at 64Ã—64

---

## â­ Final Note

This project highlights:

* Deep understanding of diffusion models
* Conditional generative modeling
* Architectural design of UNet with attention
* Training stabilization techniques
* Full generative pipeline implementation from scratch

If you find it interesting, feel free to â­ the repository.

---

## ðŸ“ˆ SEO Tags

```
Diffusion Model, DDPM, Conditional Diffusion, CelebA Diffusion, Face Generation AI, Conditional UNet, Generative AI MSc Project, PyTorch Diffusion Implementation, Noise Prediction Model, Attribute Conditioned Generation, Denoising Diffusion Probabilistic Model, Self Attention UNet, EMA Diffusion Training, Computer Vision Generative Models
```

---

## ðŸ“„ License

This project is licensed under the **MIT License**.

> Use it, build on it, experiment with it, just donâ€™t blame the diffusion process if it generates something unexpected ðŸ˜„
